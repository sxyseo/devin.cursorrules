# NeuroNest 系统架构

## 整体架构

NeuroNest采用模块化架构设计，主要包括以下核心层次：

1. **UI层**：用户界面组件，使用SwiftUI实现
2. **业务逻辑层**：核心功能模块，包括声音处理、专注管理等
3. **数据层**：用户偏好设置、历史数据和AI模型管理
4. **硬件访问层**：麦克风、摄像头、通知等系统服务访问

```
┌─────────────────────────────────────────────────┐
│                    UI 层                        │
│  ┌─────────┐  ┌─────────┐  ┌─────────────────┐  │
│  │主页视图 │  │设置视图 │  │专注统计分析视图 │  │
│  └─────────┘  └─────────┘  └─────────────────┘  │
├─────────────────────────────────────────────────┤
│                 业务逻辑层                      │
│  ┌─────────┐  ┌─────────┐  ┌─────────────────┐  │
│  │声场管理 │  │番茄时钟 │  │AI音频生成引擎   │  │
│  └─────────┘  └─────────┘  └─────────────────┘  │
├─────────────────────────────────────────────────┤
│                   数据层                        │
│  ┌─────────┐  ┌─────────┐  ┌─────────────────┐  │
│  │用户设置 │  │历史数据 │  │AI模型与参数库   │  │
│  └─────────┘  └─────────┘  └─────────────────┘  │
├─────────────────────────────────────────────────┤
│                 硬件访问层                      │
│  ┌─────────┐  ┌─────────┐  ┌─────────────────┐  │
│  │音频捕获 │  │生物信号 │  │系统事件监听器   │  │
│  └─────────┘  └─────────┘  └─────────────────┘  │
└─────────────────────────────────────────────────┘
```

## 关键技术决策

1. **编程语言与框架**
   - 主要语言：Swift 6
   - UI框架：SwiftUI，利用最新的iOS 18特性
   - 音频处理：AVFoundation + CoreAudio
   - 机器学习：CoreML + TensorFlow Lite

2. **音频处理技术**
   - 使用FFT频谱分析进行环境噪音分类
   - 相位消除技术实现软件降噪
   - 双耳节拍通过立体声差频实现
   - 本地音频合成引擎处理预加载音素

3. **AI与机器学习**
   - 轻量级CNN模型用于噪音分类(<5MB)
   - LSTM模型用于注意力状态预测
   - 端侧推理确保隐私和实时响应
   - 参数映射规则库存储脑波-音频对应关系

4. **数据存储**
   - 用户偏好：UserDefaults
   - 历史数据：CoreData
   - 音频资源：应用包内Assets + 按需下载

## 设计模式

1. **MVVM架构**
   - Model：数据模型和业务逻辑
   - ViewModel：状态管理和UI逻辑
   - View：纯展示组件

2. **观察者模式**
   - 使用Combine框架实现响应式编程
   - 发布-订阅机制处理音频状态变化

3. **策略模式**
   - 不同音频生成策略可动态切换
   - 根据用户场景选择最佳降噪算法

4. **工厂模式**
   - 音频场景工厂创建不同类型声景
   - 预设工厂管理不同类型预设配置

5. **单例模式**
   - 音频引擎管理器
   - 用户会话管理器

## 主要组件关系

1. **音频处理流水线**
   ```
   环境采集 → 噪音分析 → 降噪处理 → 音频合成 → 空间音频渲染 → 输出
   ```

2. **专注循环系统**
   ```
   状态检测 → 时间管理 → 专注/休息切换 → 打断防御 → 反馈调整
   ```

3. **AI适应系统**
   ```
   用户数据收集 → 模式识别 → 参数优化 → 音频调整 → 效果反馈
   ```

## 扩展性考虑

1. **插件系统**
   - 支持第三方声场设计
   - 自定义专注策略

2. **跨平台兼容**
   - 核心逻辑层设计考虑未来Android版本
   - 音频处理算法平台无关

3. **API集成**
   - 支持与其他生产力工具集成
   - 提供webhook用于自动化工作流 